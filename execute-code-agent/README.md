
# ü§ñüî• Code Execution Agent: Unleash the Power of AI! üî•ü§ñ

This notebook demonstrates a powerful code execution agent that leverages the capabilities of large language models (LLMs) and secure sandboxed environments to execute code generated by AI.

## üåü Overview

The agent works in the following steps:

1. **Prompting the LLM:** An LLM (in this case, Llama-3.3-70B) is prompted with a user request or instruction.
2. **Code Generation:** The LLM generates Python code designed to fulfill the user's request.
3. **Code Extraction:** The generated code is extracted from the LLM's response.
4. **Sandboxed Execution:** The code is executed within a secure sandbox environment (using E2B) to prevent potential security risks.
5. **Result Aggregation:** The output and any errors from the code execution are captured.
6. **Feedback Loop:** The output is fed back to the LLM, allowing it to refine its responses and improve subsequent code generation.

## üöÄ Getting Started

### üîë Prerequisites

*   **OpenAI API Key:** You'll need an API key from OpenAI (or in this case, a Together API key since that is the one used) to interact with their language models.
*   **E2B API Key:** An E2B API key is required to create and manage the sandboxed execution environments.
*   **Google Colab Environment:** This notebook is designed to run within the Google Colab environment, which provides convenient access to resources and libraries.

### üõ†Ô∏è Setup

1. **Install the E2B Code Interpreter Library:**

    ```bash
    !pip install e2b-code-interpreter
    ```

2. **Store API Keys as Secrets:**

    *   Use Google Colab's "Secrets" feature to securely store your `TOGETHER_API_KEY_NEW` and `E2B_API_KEY`.

## üß† Code Breakdown

### ü§ñ **1. Initializing the LLM Client**

```python
from openai import OpenAI
from google.colab import userdata

client = OpenAI(
    base_url="https://api.together.xyz/v1",
    api_key=userdata.get("TOGETHER_API_KEY_NEW")
)
```

*   We import the `OpenAI` library to interact with the LLM.
*   The `client` is initialized with the base URL for the Together API and the API key retrieved from Colab's userdata.

### üèì **2. Prompting the LLM and Getting the Initial Response**

```python
completion = client.chat.completions.create(
    model="meta-llama/Llama-3.3-70B-Instruct-Turbo-Free",
    messages=[
        {"role": "user", "content": "implement fizz buzz in python"}
    ],
)

result = completion.choices[0].message.content
print(result)
```

*   The code sends a request to the LLM (Llama-3.3-70B) to "implement fizz buzz in python".
*   The response (which includes the generated code) is stored in the `result` variable.
*   The initial result (including code and explanation) is printed.

### üîç **3. Extracting the Code Block**

```python
import re

def extract_code_block(text):
    pattern = r"```(\w*)\r?\n(.*?)\r?\n```"
    match = re.search(pattern, text, re.DOTALL)

    if match:
        language = match.group(1).lower() or None
        code = match.group(2)
        return language, code

    return None, None

print(extract_code_block(result)[0])  # Prints the language (e.g., "python")
print(extract_code_block(result)[1])  # Prints the extracted code
```

*   The `extract_code_block` function uses a regular expression to find and extract code blocks enclosed in triple backticks (```).
*   It returns the programming language (if specified) and the code itself.

### ü•™ **4. Setting up the Sandboxed Environment**

```python
from e2b_code_interpreter import Sandbox

extracted_code = extract_code_block(result)[1]
sbx = Sandbox(
    api_key=userdata.get("E2B_API_KEY")
)
```

*   The `Sandbox` class from the `e2b_code_interpreter` library is used to create a secure environment.
*   The sandbox is initialized with the E2B API key.

### ‚öôÔ∏è **5. Running the Code and Retrieving Results**

```python
exexution = sbx.run_code(extracted_code)
print(exexution.logs)
sbx.kill()  # Terminate the sandbox

print(exexution.logs.stdout[0])
```

*   `sbx.run_code()` executes the extracted Python code within the sandbox.
*   The `exexution` object contains the output (stdout and stderr) and any errors.
*   `sbx.kill()` terminates the sandbox after execution.
*   We print the logs and specifically the standard output of the executed code.

### üîÅ **6. The Interactive Loop**

```python
chat_history = [
    {"role": "system", "content": """
        Write Python code that you can run to answer the user's requests.
        Always trust the output of the code (assistant) as true and summarize the output of the code to the user.
        """},
    {"role": "user", "content": "Tell me about the GitHub user nasserml. You can use the GitHub API. What do you think about his coding skills?"}
]

sbx = Sandbox(api_key=userdata.get("E2B_API_KEY"))

while True:
    completion = client.chat.completions.create(
        model="meta-llama/Llama-3.3-70B-Instruct-Turbo-Free",
        messages=chat_history
    )

    response = completion.choices[0].message.content
    chat_history.append({"role": "system", "content": response})
    language, code = extract_code_block(response)

    print("============================================resp")
    print(response)
    print("="*100)

    if code:
        execution = sbx.run_code(code)
        chat_history.append({"role": "assistant", "content": "I have run the code and the output of the code is:\n" + execution.logs.stdout[0][:5000]})
        print("==================================code exe sndbox ==========================================================")
        print(execution.logs.stdout[0])

        if execution.error:
            print("Error ==============================================")
            print(execution.error)
            chat_history.append({"role": "assistant", "content": execution.error})
    else:
        sbx.kill()
        break

chat_history
```

*   **Chat History:** The `chat_history` list stores the conversation with the LLM, including system prompts, user requests, and the LLM's responses.
*   **Loop:**
    *   The `while True` loop continues until the LLM generates a response without code, indicating the end of the interaction.
    *   The LLM is prompted with the `chat_history`.
    *   The response is added to `chat_history`.
    *   Code is extracted.
    *   If code exists:
        *   It's executed in the sandbox.
        *   The output is added to `chat_history` as an assistant message.
        *   Any errors are handled and added to the `chat_history` as well.
    *   If no code exists:
        *   The sandbox is killed.
        *   The loop breaks.

### **7. Printing the Chat History**

*   Finally, the complete `chat_history` is printed, showing the entire conversation and the interaction between the user, the LLM, and the code execution agent.

## ü§© Unique and Awesome Features

*   **Emoji-Enhanced:** üéâ We've sprinkled emojis throughout the Markdown to make it more engaging and fun!
*   **Clear Sectioning:** The use of headings, subheadings, and bullet points makes the documentation easy to navigate and understand.
*   **Code Highlighting:** Code blocks are properly formatted for readability.
*   **Explanations:** Each code section is accompanied by clear explanations of its purpose and functionality.
*   **Interactive Example:** The FizzBuzz example and the GitHub user information retrieval example demonstrate the practical use of the agent.

## üåà Conclusion

This code execution agent is a powerful tool that showcases the potential of combining LLMs with secure code execution. It can be used for a wide range of tasks, from simple code snippets to complex data analysis and API interactions. The feedback loop allows the agent to learn and improve over time, making it a valuable asset for developers and researchers alike.

## üîÆ Future Enhancements

*   **Error Handling:** Implement more robust error handling to provide more informative feedback to the LLM and the user.
*   **State Management:** Explore ways to manage state across multiple code executions, allowing the agent to build upon previous results.
*   **Support for Other Languages:** Extend the agent to support other programming languages beyond Python.
*   **User Interface:** Develop a user-friendly interface to make the agent more accessible to a wider audience.

