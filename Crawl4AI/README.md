# âœ¨ Crawl4AI Examples: Unleash the Power of Web Crawling! ğŸ•¸ï¸

Dive into these amazing examples demonstrating the capabilities of Crawl4AI! ğŸš€  We'll explore single page crawling, sequential processing, and lightning-fast parallel crawling. Get ready to be impressed! ğŸ¤©

### **1. `1-crawl_single_page.py`**  ğŸ”ğŸ“„

**Purpose:**  Crawls a single web page (a Google search results page in this case) and prints the extracted content in Markdown format.

**Code Breakdown:**

```python
import asyncio  # ğŸ”„ For asynchronous programming
from crawl4ai import *  # ğŸ•·ï¸ Import the Crawl4AI library

async def main():
    async with AsyncWebCrawler() as crawler:  # ğŸ•¸ï¸ Create a crawler instance
        result = await crawler.arun(
            url="https://www.google.com/search?q=20-20+voice+cancer+uk"  # ğŸ¯ The URL to crawl
        )
        print(result.markdown)  # ğŸ–¨ï¸ Print the Markdown output

if __name__ == "__main__":
    asyncio.run(main())  # â–¶ï¸ Run the main function
```

**Emojis in Action:**

*   ğŸ” **Search:** This script performs a search (on Google) and extracts information.
*   ğŸ“„ **Single Page:** It focuses on crawling only one specific page.
*   ğŸ”„ **Asynchronous:** Uses `asyncio` for efficient, non-blocking operations.
*   ğŸ•·ï¸ **Crawler:** The `crawl4ai` library is the star of the show, handling the web crawling.
*   ğŸ¯ **Target URL:** Specifies the exact web address to be processed.
*   ğŸ–¨ï¸ **Print Output:** Displays the result (Markdown content) on the console.
*   â–¶ï¸ **Run:** Executes the code.

### **2. `2-crawl_docs_sequential.py`** ğŸ“šğŸš¶â€â™‚ï¸

**Purpose:** Crawls multiple pages from a website (Pydantic AI documentation) sequentially, one after another, and prints the length of the extracted Markdown content for each page.

**Code Breakdown:**

```python
import asyncio  # ğŸ”„ Asynchronous operations
from typing import List  # ğŸ“ Type hinting
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig  # ğŸ•¸ï¸ Crawl4AI components
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator  # âœï¸ Markdown generator
import requests  # ğŸŒ For fetching the sitemap
from xml.etree import ElementTree  # ğŸŒ³ For parsing XML

async def crawl_sequential(urls: List[str]):
    print("\n=== Sequential Crawling with Session Reuse ===")  # ğŸš¶â€â™‚ï¸â¡ï¸ğŸš¶â€â™‚ï¸ Sequence

    browser_config = BrowserConfig(
        headless=True,  # ğŸ‘» Run browser in the background
        extra_args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"],  # âš™ï¸ Optimization flags
    )

    crawl_config = CrawlerRunConfig(
        markdown_generator=DefaultMarkdownGenerator()  # âœï¸ Default Markdown style
    )

    crawler = AsyncWebCrawler(config=browser_config)  # ğŸ•¸ï¸ Create the crawler
    await crawler.start()  # â–¶ï¸ Start the crawler

    try:
        session_id = "session1"  # ğŸªª Reuse session for efficiency
        for url in urls:
            result = await crawler.arun(
                url=url,
                config=crawl_config,
                session_id=session_id
            )
            if result.success:
                print(f"Successfully crawled: {url}")  # âœ… Success message
                print(f"Markdown length: {len(result.markdown_v2.raw_markdown)}")  # ğŸ“ Markdown length
            else:
                print(f"Failed: {url} - Error: {result.error_message}")  # âŒ Error message
    finally:
        await crawler.close()  # â¹ï¸ Close the crawler

# ... (rest of the code for fetching URLs from sitemap)
```

**Emojis in Action:**

*   ğŸ“š **Documentation:** This script targets a documentation website.
*   ğŸš¶â€â™‚ï¸ **Sequential:** Processes pages one by one, in order.
*   ğŸ”„ **Asynchronous:** Still uses `asyncio`, but the crawling is sequential within the loop.
*   ğŸ“ **Type Hinting:** Uses `List[str]` for better code clarity.
*   ğŸ•¸ï¸ **Crawl4AI:**  Employs various components like `AsyncWebCrawler`, `BrowserConfig`, and `CrawlerRunConfig`.
*   âœï¸ **Markdown Generator:** Specifies how to format the extracted content into Markdown.
*   ğŸŒ **Requests:**  Uses the `requests` library to download the website's sitemap.
*   ğŸŒ³ **XML Parsing:**  Parses the sitemap (which is in XML format) to extract the URLs.
*   ğŸš¶â€â™‚ï¸â¡ï¸ğŸš¶â€â™‚ï¸ **Sequence Indicator:**  Highlights the sequential nature of the crawling process.
*   ğŸ‘» **Headless Browser:** Runs the browser without a visible window.
*   âš™ï¸ **Optimization:** Uses browser flags for improved performance, especially in restricted environments (like Docker).
*   ğŸªª **Session Reuse:** Reuses the browser session to avoid restarting it for each page, making the process faster.
*   âœ… **Success:** Indicates a successful crawl.
*   ğŸ“ **Markdown Length:** Reports the length of the generated Markdown for each page.
*   âŒ **Error:** Signals a failure during crawling.
*   â¹ï¸ **Close:**  Ensures the crawler is properly shut down after processing all URLs.

### **3. `3-crawl_docs_FAST.py`** ğŸ“šğŸš€

**Purpose:** Crawls multiple pages from a website (Pydantic AI documentation) in parallel (concurrently), significantly speeding up the process. It also includes memory usage monitoring.

**Code Breakdown:**

```python
# ... (imports and setup)

async def crawl_parallel(urls: List[str], max_concurrent: int = 3):
    print("\n=== Parallel Crawling with Browser Reuse + Memory Check ===")  # ğŸƒâ€â™‚ï¸ğŸƒâ€â™‚ï¸ğŸƒâ€â™‚ï¸ Parallel

    peak_memory = 0  # ğŸ“ˆ Track peak memory usage
    process = psutil.Process(os.getpid())  # ğŸ’½ Get current process info

    def log_memory(prefix: str = ""):
        # ... (function to log current and peak memory)

    browser_config = BrowserConfig(
        # ... (minimal browser configuration)
    )
    crawl_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)  # ğŸ”€ Bypass caching

    crawler = AsyncWebCrawler(config=browser_config)  # ğŸ•¸ï¸ Create crawler
    await crawler.start()  # â–¶ï¸ Start crawler

    try:
        success_count = 0  # ğŸ‘ Successful crawls
        fail_count = 0  # ğŸ‘ Failed crawls
        for i in range(0, len(urls), max_concurrent):
            batch = urls[i : i + max_concurrent]
            tasks = []

            for j, url in enumerate(batch):
                session_id = f"parallel_session_{i + j}"  # ğŸªª Unique session per task
                task = crawler.arun(url=url, config=crawl_config, session_id=session_id)
                tasks.append(task)

            log_memory(prefix=f"Before batch {i//max_concurrent + 1}: ")  # ğŸ“Š Log memory before

            results = await asyncio.gather(*tasks, return_exceptions=True)  # ğŸ¤ Gather results

            log_memory(prefix=f"After batch {i//max_concurrent + 1}: ")  # ğŸ“Š Log memory after

            # ... (evaluate results and print summary)

    finally:
        # ... (close crawler and log final memory usage)

# ... (rest of the code for fetching URLs from sitemap and main function)
```

**Emojis in Action:**

*   ğŸ“š **Documentation:** Still targets a documentation website.
*   ğŸš€ **Fast:** This is the key difference - it's designed for speed through parallel processing.
*   ğŸƒâ€â™‚ï¸ğŸƒâ€â™‚ï¸ğŸƒâ€â™‚ï¸ **Parallel:** Processes multiple pages concurrently.
*   ğŸ“ˆ **Peak Memory:** Keeps track of the highest memory usage during the process.
*   ğŸ’½ **Process Info:**  Uses `psutil` to get information about the running process (for memory tracking).
*   ğŸ“Š **Memory Logging:**  Logs memory usage before and after each batch of URLs.
*   ğŸ”€ **Bypass Cache:**  Disables caching to ensure fresh data is fetched each time.
*   ğŸ‘ **Success Count:** Counts the number of successful crawls.
*   ğŸ‘ **Fail Count:** Counts the number of failed crawls.
*   ğŸªª **Unique Session:**  Each concurrent task gets its own session ID.
*   ğŸ¤ **Gather Results:**  `asyncio.gather` collects the results from all the parallel tasks.

### **Summary Table**

| Feature         | `1-crawl_single_page.py` | `2-crawl_docs_sequential.py` | `3-crawl_docs_FAST.py` |
| :-------------- | :----------------------- | :--------------------------- | :--------------------- |
| **Purpose**     | Crawl a single page      | Crawl multiple pages sequentially | Crawl multiple pages in parallel |
| **Speed**       | Slow                     | Moderate                     | **Fast**               |
| **Concurrency** | None                     | None                         | **Yes (up to `max_concurrent`)** |
| **Session**     | Single                   | Reused                       | **Unique per task**      |
| **Memory Check**| No                       | No                           | **Yes**                |
| **Caching**     | Default                  | Default                      | **Bypass**             |
| **Emojis**      | ğŸ”ğŸ“„ğŸ”„ğŸ•·ï¸ğŸ¯ğŸ–¨ï¸â–¶ï¸           | ğŸ“šğŸš¶â€â™‚ï¸ğŸ”„ğŸ“ğŸ•¸ï¸âœï¸ğŸŒğŸŒ³ğŸš¶â€â™‚ï¸â¡ï¸ğŸš¶â€â™‚ï¸ğŸ‘»âš™ï¸ğŸªªâœ…ğŸ“âŒâ¹ï¸ | ğŸ“šğŸš€ğŸƒâ€â™‚ï¸ğŸƒâ€â™‚ï¸ğŸƒâ€â™‚ï¸ğŸ“ˆğŸ’½ğŸ“ŠğŸ”€ğŸ‘ğŸ‘ğŸªªğŸ¤ |

